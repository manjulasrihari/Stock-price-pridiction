{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml,html\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "path ='C:\\\\Users\\\\HP\\\\Downloads\\\\Compressed\\\\aclImdb\\\\train\\\\pos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = 'C:\\\\Users\\\\HP\\\\Downloads\\\\Compressed\\\\aclImdb\\\\train\\\\neg'\n",
    "allfiles = glob.glob(path + \"\\*.txt\")\n",
    "\n",
    "mylist =[]\n",
    "for myfile in allfiles:\n",
    "    print(myfile)\n",
    "    df = pd.read_csv(myfile,error_bad_lines=False,low_memory=True,quoting=0,header=None,skip_blank_lines=True,sep='\\r\\t',encoding='Latin-1')\n",
    "    mylist.append(df)\n",
    "\n",
    "    \n",
    "df_neg = pd.concat(mylist,axis=0,ignore_index=True)\n",
    "df_neg.columns=['review']\n",
    "df_neg['label']='negative'\n",
    "df_neg.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = 'C:\\\\Users\\\\HP\\\\Downloads\\\\Compressed\\\\aclImdb\\\\train\\\\pos'\n",
    "allfiles = glob.glob(path + \"\\*.txt\")\n",
    "\n",
    "mylist =[]\n",
    "for myfile in allfiles:\n",
    "    print(myfile)\n",
    "    df = pd.read_csv(myfile,error_bad_lines=False,low_memory=True,quoting=0,header=None,skip_blank_lines=True,sep='\\r\\t',encoding='Latin-1')\n",
    "    mylist.append(df)\n",
    "\n",
    "    \n",
    "df_pos = pd.concat(mylist,axis=0,ignore_index=True)\n",
    "df_pos.columns=['review']\n",
    "df_pos['label']='positive'\n",
    "df_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.concat([df_neg,df_pos])\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "train_df['review'] = [BeautifulSoup(text).get_text() for text in train_df['review']]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path1 = 'C:\\\\Users\\\\HP\\\\Downloads\\\\Compressed\\\\aclImdb\\\\train\\\\pos'\n",
    "path2 = 'C:\\\\Users\\\\HP\\\\Downloads\\\\Compressed\\\\aclImdb\\\\train\\\\pos'\n",
    "allfilesneg = glob.glob(path1 + '\\*.txt')\n",
    "allfilespos = glob.glob(path2 + '\\*.txt')\n",
    "mylistneg = []\n",
    "mylistpos = []\n",
    "for myfilepos in allfilespos:\n",
    "    \n",
    "    dfpos = pd.read_csv(myfilepos,error_bad_lines=False,low_memory=True,quoting=0,header=None,skip_blank_lines=True,sep='\\r\\t',encoding='Latin-1')\n",
    "    mylistpos.append(dfpos)\n",
    "\n",
    "    \n",
    "for myfileneg in allfilesneg:\n",
    "    \n",
    "    dfneg = pd.read_csv(myfileneg,error_bad_lines=False,low_memory=True,quoting=0,header=None,skip_blank_lines=True,sep='\\r\\t',encoding='Latin-1')\n",
    "    mylistneg.append(dfneg)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "df_pos = pd.concat(mylistpos, axis = 0,ignore_index=True)\n",
    "\n",
    "df_pos.columns=['review']\n",
    "df_pos['label']='positive'\n",
    "   \n",
    "df_neg = pd.concat(mylistneg, axis = 0,ignore_index=True)\n",
    "df_neg.columns=['review']\n",
    "df_neg['label']='negative'\n",
    "\n",
    "\n",
    "test_df=pd.concat([df_neg,df_pos])\n",
    "\n",
    "test_df['review'] = [BeautifulSoup(text).get_text() for text in test_df['review']]\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['review'] = train_df['review'].str.lower().str.replace('[^a-z]','').str.split()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['review'] = test_df['review'].str.lower().str.replace('[^a-z]','').str.split()\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#stopwords = stopwords.words('english')\n",
    "#stopwords.extend(['movie','film','story','seen','see','saw','get','also','went','night','last','character','people','would','get'])\n",
    "#df['Text'] = df['Text'].apply(lambda x: [word for word in x if word not in set(stopwords.words('english'))])\n",
    "#df\n",
    "\n",
    "train_df['review'] = train_df['review'].apply(lambda x: [word for word in x if word not in set(stopwords.words('english'))])\n",
    "train_df.head()\n",
    "\n",
    "#test_df['review'] = test_df['review'].apply(lambda x: [word for word in x if word not in set(stopwords.words('english'))])\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['review'] = test_df['review'].apply(lambda x: [word for word in x if word not in set(stopwords.words('english'))])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "train=train_df.copy()\n",
    "\n",
    "train['review'] = train['review'].apply(lambda x:''.join([ps.stem(word) for word in x]))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "test=test_df.copy()\n",
    "\n",
    "test['review'] = test['review'].apply(lambda x:''.join([ps.stem(word) for word in x]))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()\n",
    "\n",
    "df = pd.concat([train_df,test_df],ignore_index = True)\n",
    "cv = CountVectorizer()\n",
    "X_df = cv.fit_transform(df['review'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y_df = le.fit(X_df,Y_df)\n",
    "\n",
    "tree_clf = ExtraTreesClassifier()\n",
    "tree_clf.fit(X_df,Y_df)\n",
    "\n",
    "importances = tree_clf.feature_importances_\n",
    "feature_names = cv.get_feature_names()\n",
    "feature_imp_dict = dict(zip(feature_names,importances))\n",
    "sorted_features = sorted(feature_imp_dict.items(), reverse=True)\n",
    "model = SelectFromModel(tree_clf, Prefit=True)\n",
    "X_df_updated = model.transform(X_df)\n",
    "print('Total feature count', X_df.shape[1])\n",
    " print('Selected feature', X_df_updated.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download()\n",
    "from nltk.corpus import wordnet as wn\n",
    "#from nltk.corpus import wordnet\n",
    "lm = WordNetLemmatizer()\n",
    "trainlm = train_df.copy()\n",
    "testlm = test_df.copy()\n",
    "\n",
    "trainlm['review']= trainlm['review'].apply(lambda x: ' '.join([lm.Lemmatize(word) for word in x]))\n",
    "trainlm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlm['review'] = testlm['review'].apply(lambda x: ' '.join([lm.Lemmatize(word) for word in x ]))\n",
    "testlm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "path = 'C:\\\\Users\\\\HP\\\\Downloads\\\\Compressed\\\\aclImdb\\\\train\\\\unsup'\n",
    "allfiles = glob.glob(path +\"\\*.txt\")\n",
    "\n",
    "mylist = []\n",
    "\n",
    "\n",
    "for myfile in allfiles:\n",
    "    print(myfile)\n",
    "    dfs = pd.read_csv(myfile,error_bad_lines=False,low_memory=True,header=None,skip_blank_lines=True,sep='\\r\\t',encoding='Latin-1')\n",
    "    mylist.append(dfs)\n",
    "    \n",
    "    \n",
    "unsup_df= pd.concat(mylist,axis = 0,ignore_index=True)\n",
    "unsup_df.columns=['review']\n",
    "unsup_df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans = kmeans(n_clusters=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the html tags using regular expression, all tags start with '<' and end with '>'\n",
    "#since the data has tags with space in between and gets separated\n",
    "def remove_htmltags(string_input):\n",
    "    string_out=[]\n",
    "    for word in re.split(r'(?:<.*?>|\\s)+', string_input):\n",
    "        string_out.append(word)\n",
    "    string_out = filter(None, string_out)#filters out the empty string in list\n",
    "    return ' '.join(list(string_out))#list of all the words in string filtering out html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_df['review']= unsup_df['review'].str.lower().apply(lambda x : remove_htmltags(x))\n",
    "unsup_df['review']= unsup_df['review'].replace(regex=r'[^a-z]', value='')\n",
    "unsup_df['review']= unsup_df['review'].str.split()\n",
    "                                               \n",
    "                                               \n",
    "unsup_df['review'] = unsup_df['review'].apply(lambda x:' '.join([word for word in x if len(word)>1]))\n",
    "unsup_df.loc[:5,'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "cache_stopwords = stopwords('english')\n",
    "unsup_df['review'] = unsup_df['review'].str.split()\n",
    "\n",
    "unsup_df.loc[:,'review'] = unsup_df.loc[:,'review'].apply(lambda x: [word for word in x if word not in set(cache_stopwords)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_train_pos = pd.Series(' '.join((train_df[train_df['label'] == 'pos']['review'])).split()).value_counts()[:20]\n",
    "most_common_train_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_train_neg = pd.Series(' '.join((train_df[train_df['label'] == 'pos']['review'])).split()).value_counts()[:20]\n",
    "most_common_train_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_test_pos = pd.Series(' '.join((train_df[train_df['label'] == 'pos']['review'])).split()).value_counts()[:20]\n",
    "most_common_test_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_test_neg = pd.Series(' '.join((train_df[train_df['label'] == 'pos']['review'])).split()).value_counts()[:20]\n",
    "most_common_test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "vect_train = vect.fit_transform(train_df['review'])\n",
    "\n",
    "train_freq = pd.DataFrame(vect_train.sum(axis=0),columns = list(vect.get_feature_names()),index = ['highest_freaquency']).T\n",
    "train_freq.largest(20,'highest_grequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['review', 'label']]\n",
    "train_df.rename(columns={'review':'target', 'label':'text'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[train_df[\"target\"] == \"train\"][\"text\"].unique().tolist()\n",
    "train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = \" \".join(train)\n",
    "train_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_wordcloud = WordCloud().generate(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_list_wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempcv=trainlm.copy()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvp = CountVectorizer(max_df=0.95,min_df=0.05)\n",
    "vecp=cvp.fit_transform(tempcv.review[tempcv.label=='positive'])\n",
    "cvp.vocabulary_\n",
    "\n",
    "freqs = zip(cvp.get_feature_names(),vecp.toarray().sum(axis=0))\n",
    "print('Positive review Words with their frequencies in corpus->')\n",
    "print(sorted(freqs,key=lambda x: x[1],reverse=True[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvn = CountVectorizer(max_df=0.95,min_df=0.05)\n",
    "vecn=cvn.fit_transform(tempcv.review[tempcv.label=='nagative'])\n",
    "cvn.vocabulary_\n",
    "\n",
    "freqs = zip(cvn.get_feature_names(),vecn.toarray().sum(axis=0))\n",
    "\n",
    "print('Negative review words with their frequencies in corpus->')\n",
    "\n",
    "print(sorted(freqs, key=lambda x: x[1],reverse=True)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temptf=trainlm.copy()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfvectorizerp = TfidfVectorizer(min_df=0.1,max_df = 0.9,sublinear_tf=True,use_idf=True)\n",
    "tvp = tfvectorizerp.fit_transform(temptf.review[temptf['label']=='positive'])\n",
    "tfvectorizer.vocabulary_\n",
    "\n",
    "t=tfvectorizerp.vocabulary_\n",
    "pos_words=[]\n",
    "\n",
    "for key in t:\n",
    "    pos_words.append(key)\n",
    "    \n",
    "pos=\"\".join(pos_words)\n",
    "pos[:1000]\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "pos_wordcloud = WordCloud().generate(pos)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.imshow(pos_WordCloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
